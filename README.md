# Rule-Based
# Rule-Filter_based# Rule-Based
# Installation & Dependencies

## Backend (Flask)

### Install Dependencies
```sh
pip install flask flask-cors
```

### Run the Backend
```sh
python app.py
```

## Frontend (React + Vite)

### Create a Vite Project
```sh
npm create vite@latest frontend --template react
cd frontend
```

### Install Dependencies
```sh
npm install
```

### Set Up Tailwind CSS
```sh
npm install -D tailwindcss postcss autoprefixer
npx tailwindcss init -p
```

Modify `tailwind.config.js`:
```js
export default {
  content: ["./index.html", "./src/**/*.{js,ts,jsx,tsx}"],
  theme: {
    extend: {},
  },
  plugins: [],
};
```

Modify `src/index.css`:
```css
@tailwind base;
@tailwind components;
@tailwind utilities;
```

### Run the Frontend
```sh
npm run dev

```
# ğŸ§  RAG Customer Intelligence Assistant

A Retrieval-Augmented Generation (RAG) system that allows natural language querying over a structured customer dataset using FAISS, HuggingFace embeddings, and a locally running LLaMA 3.2 model via Ollama.

---

## ğŸ“¦ Features

- ğŸ” Ask **any question** about 500 customers â€” based on **email**, **address**, **avatar color**, **time on app**, **time on website**, **membership length**, and **yearly spend**.
- ğŸ§  RAG-powered: Uses vector embeddings of structured customer profiles for high-relevance answers.
- ğŸ“„ Returns **formatted profiles with auto-generated summaries**.
- ğŸ§¾ Includes **source document excerpts** for transparency.
- âš™ï¸ Powered by **LangChain**, **FAISS**, **MiniLM**, and **Ollama (LLaMA 3.2)**.

---

## ğŸ“ Project Structure

